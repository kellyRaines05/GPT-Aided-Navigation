# GPT-Aided-Navigation
This project aims to enhance navigation for the visually impaied and blind by providing information about their immediate surroundings, up to at least twenty feet ahead of them. Vision language models such as GPT-4o brings a unique solution in daily situations users face when trying to envision their surroundings, details missed below/above detection by a walking stick or similar solutions currently being used.

## Setup simulate_gpt-4o_nav
1. Get Open-AI key to access Chat-GPT API
2. Add to environment system variable: OPEN_API_KEY followed by your api key
3. Download the prebuild of Carla [here](https://github.com/carla-simulator/carla/releases/tag/0.9.15/) for Ubuntu or Windows systems
4. Launch carla_0.9.15/CarlaUE4
5. Run carla_script.py
